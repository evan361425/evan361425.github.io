---
description: 熱門的維運書籍：Release It!，的相關心得和整理，帶你了解維運會遇到的問題、解決方法、架構設計、心態調整等等。
image: https://i.imgur.com/CToGhtB.jpg
---

# Release It

![Release It! 的封面](https://i.imgur.com/CToGhtB.jpg)

1. 目的端在虛擬 IP 之下更改主機 IP 造成 TCP 連線中斷，應用程式卻在關閉連線前出現未處理的例外狀況，導致連線池中的所有連線都被迫停擺（為了等待上一個請求完成）
    -   避免連鎖式反應影響其他服務，例如艙壁（bulkheads）設計原則，replica 資料給 readonly，然後使用事件機制做寫入
2. TCP（資料庫的）連線被防火牆丟棄（注意，不是拒絕），導致來源目的端都認為連線仍存在，實際卻無法進行任何溝通
    -   TCP keepalive，確保防火牆允許連線
    -   逾時 + 重啟機制
3. 在高可用性下的多個節點，當服務有著 *會隨著流量上升而變明顯的錯誤* 時（例如記憶體堆積），單一節點的失能會加速其他節點的失能，在未獲得改善前（通常記憶體堆積的錯誤並不明顯）可能需要定期去重啟服務
    -   可以使用艙壁設計原則，例如 Hash(IP) 後分流到不同節點群（單一資料中心），即使某節點群全部失能，也只會影響一部份的使用者
    -   auto scaling
4. 目的端並未告知錯誤是因為暫時性錯誤（例如鎖死）還是長期錯誤（例如連線），所以來源端反覆重新發送，加速雙方的消亡
    -   注意連線池中的連線生滅和停擺
    -   逾時 + 重啟機制
    -   提早把錯誤回給來源端，避免加重後端服務的負載
5. 前端使用者錯誤設置的代理者反覆發送請求而不紀錄 session ID，導致反覆建置 session
    -   建置防禦 DDoS 機制
    -   艙壁設計原則，避免服務的失能
6. 斷路器導致跳電，每台主機在重開機的過程中請求高電流，導致斷路器反覆跳電
    -   逐台開機
    -   插上螺絲起子避免斷路器跳電，同時加強冷氣機和風扇避免過熱
7. 套件管理工具在服務升版過程把預期被關掉的 autoscaler 啟動，導致 autoscaler 因為過時的錯誤狀態，關閉大部分節點。—80
    -   非預期時間降載時，通知管理者
    -   做大規模更動時，需要手動介入
    -   確保熱機的過程
8. 資料庫擁有超乎預期的資料時，在沒有使用 pagination 時，會吃光節點的記憶體。—87
    -   不要相信外部資料
    -   確保限制應用層協定傳輸的大小
9. 當物件運算錯誤時，需要丟出錯誤，而非繼續讓該物件接著進行後續工作後再丟錯。—107
    -   預先要到應有的資源，當無法得到時，就先丟錯，不進行運算
10. 當其中一個下游服務能處理的量變很低，會導致上游服務完全失能，因為連線池的連線是共用的。其外顯特徵包括：頻寬很高、延時很長、資源使用率低
    -   哈

---

做一台 server，他會模擬一些底層運作的錯誤：

-   拒絕連線
-   排進的隊裡，但是不被執行
-   只會回 `SYN`/`ACK` 的訊號
-   （建立連線前或後）只會回 `RESET`
-   建立連線後不送資料
-   建立連線後一直不回 `ACK`，導致一直重新發送
-   只會回 HTTP Header

---

## 線上環境

當線上問題出錯時，在應用程式中加上 log 可能不是最快最好的辦法，你會需要進到機器中進行診斷。
可以診斷的前提是：你要對你的應用程式和其運行環境有足夠的瞭解，以下分幾個面向：

-   電腦和網路
-   單一節點
-   叢集
-   叢集控制（或者 control plan）
-   維運

### 電腦和網路

主要分兩類，網路和運行控制：

-   網路：包括主機名（hostname）、IP 等等。
-   運行控制（runtime control）：實體機、虛擬機、容器、雲端。

#### 網路

*Domain* 是外部網址，透過 DNS 解析 IP 後路由到指定節點的名稱；
*Hostname* 是主機名稱，一台主機只會有一個名稱，你可以透過下指令 `hostname` 獲得。
兩者合在一起稱為 *完整網域名稱*（Fully Qualified Domain Name, FQDN）。

換句話說，一台機器可以有很多個 Domain/FQDN，而且 Hostname 可能會和 Domain 不一樣。

除此之外，在開發環境中 NIC 可能有多個，例如：

```bash=
$ ifconfig 
lo0: <loopback>
gif0: <tunnel for ipv4,6>
stf0: <tunnel for ipv4,6>
ap1: <access-point mostly for WiFi>
en0: <ethernet>
en*: <more ethernet>
awdl0: <apple wireless direct link for somthing like AirDrop>
bridge0: <for virtual>
llw0: <low-latency WLAN>
utun0: <tunnel for VPN>
utun*: <more tunnel>
```

但在線上環境可能就只會有一個是給應用程式連線用、一個是給後台管理或資料備份用，這兩個接口可能各自有獨立的 IP（這時很可能就對應到不同的 domain）。
也可能是一個應用程式兩個 NIC 但都是相同的 IP，這時使用的技術稱為搭接（bonding/teaming），目的是分攤出去的流量。
也因為這樣，有時在建立應用程式的時候我們不能綁定所有的 NIC（`0.0.0.0:8080`）而是要指定 domain 或 IP（`app.example.com:8080`）。

#### 運行控制

分為三類實體機（physical host）、虛擬機（virtual machine）、容器（container）。

##### 實體機

一般來說和開發環境並不會相差太多，都是多核心、x86、64 位元、相似的時鐘晶片。
主要差異可能在於資料中心的主機儲存空間通常不會太大，他通常會透過 NAS 或 SAN 來擴充。
這是為了讓單台機器的成本降低，讓水平擴展可以節省地被達成。
除此之外，如果有特殊應用需要使用到 GPU 或高 RAM（例如機器學習、圖形運算）才會額外賦予該應用特殊機器。

##### 虛擬機

現在的網路應用節點毫無疑問是以虛擬機作為主導，雖然犧牲了一些資源都是卻換來了很大的管理方便。
但是虛擬機還是有些問題，例如他的效能是難預期的，這裡包括 CPU、記憶體、網路。
這是因為掛載虛擬機的主機（host），會為了資源調度而暫停這些虛擬機的運作。

這聽起來可以被接受，因為網路應用的延時一直都是難以預期的，但如果那些被迫暫停的節點是重要的服務，爆炸半徑可能就會很大了。
例如管理叢集的節點，如 auto-scaling、服務發現或共識演算。

除此之外，時鐘的偏移在這個環境下，產生了更大的變數。
虛擬機會為了和 Host 對齊時鐘而強制調整時鐘，對應用來說，時間就可能會亂跳（會往前也會往後），如果應用是對時間敏感的，就需要注意。

##### 容器

容器通常是開發者需要去設計和調整的運行控制，相對而言虛擬機則是系統管理員需要去處理的。
容器很像在雲端上管理虛擬機，你不會預期他的 IP 不會被改變也不會把重要的資料放進其中的資料系統，因為它通常是短暫存在的。

容器減少了開發環境和線上環境的差異，但他仍有一些困境，不過隨著其發展，這些困境已經一一被解決了。
不過在使用上仍需要注意一些事情：

-   容器預期是快速生滅的，服務應該避免過久的啟動和關閉。
-   除錯是困難的，如果你有發生過線上問題，你會發現進去容器後簡直一籌莫展，因為裡面的環境乾淨到很難做些什麼事情。
你會需要一些時間去適應容器的除錯。

:::spoiler 容器的困境
網路在容器世界是複雜的，因為你可能不會在 Host 上暴露他的網路埠，但卻需要讓他有能力對外連線（換句話說，只出不進）。
我們通常會使用 VLAN（或者說，overlay network）去橋接這個連線，並用軟體交換器去交換封包。

除此之外，容器通常是小而多的，所以你會需要一個自動化的管理系統（或者說，control plane）。
:::

##### 雲端

雖然你需要花些時間搬遷應用到雲端上，但是雲端環境提供很多優勢，最主要的就是可用性（適合做 auto-scaling）和低成本。但是在雲端：

-   你的虛擬機可能會因為營運方的管理等因素，[被要求重新啟動](https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-automatic-reboot-cause/)。
-   就像容器一樣，你在雲端上的機器不會有固定的 IP，除非花錢去租賃。
-   通常一台機器配上一個（虛擬）NIC，所以你只會拿到一個私有 IP，但有時應用的需求需要多張 NIC。

雲端上的容器面臨著容器和雲端上的虛擬機會有的困境。
但是隨著雲端服務的成熟，這些困難其實都不是困難，只是會需要你花點時間去研究和累積維運經驗。

### 單一節點

單一節點雖然是一個龐大服務的基石，但是建構良好的節點，會幫助你在後續維運大型服務省下很多功夫。

部署和設定總結幾點注意：

-   程式碼要放在版本控制（version control）中，別塞機敏資訊。
-   部署要自動化，並確保依賴和插件的安全性。
-   比起運行控制隨著歷程改變，每次改變都是從單一點出發更為安全：
  ![每次更新都從 Base Image 延伸，避免複雜化狀態](https://i.imgur.com/b2xyD6V.png)
-   小服務設定檔用注入（檔案或環境變數）；大（多個微）服務可以用專門服務來替代（Consul/ZooKeeper/etcd）。

監控總結幾點注意：

-   在應用設計之初就要建立日誌（log）、測度（metrics）和監控（alert）的架構。
-   避免日誌、測度的輸出和觀測之間的耦合化，也就是在觀測新指標或新監控時，不需要改應用程式。
-   寫日誌是最直接觀察應用的行為的方式，有幾點注意：
    -   日誌位置最好在應用的位置之外（`/var/logs`），容器的話就單純輸出到 `stdout` 就可以。
    -   不要讓 `error` 層級的日誌常態出現（例如使用者輸入格式錯誤不應為 `error`）。
    -   寫清楚一點，因為緊急狀況會讓判斷力下降。
    -   單一請求的日誌要有 ID 標示。
-   暴露健康檢查接口，包括應用程式的連線狀況、IP、版本資訊。

### 叢集

隨著流量變多或為了高可用性（High Availability, HA），一個服務開始從單一節點成長為一個叢集。
這時我們看待服務就不是從節點的角度去看，而是一個由服務發現（service dicovery）、負載平衡（load balance）組成的叢集。

這類工具很多，從傳統的單一職責的 ZooKeeper、Consul、Nginx，
到現在全部整合的 Kubernetes（或有點過時的但是才出生十來年的 Mesos）。
我們在使用這些工具的時候，**要考量從公司的規模、服務的架構到這些工具本身的迭代性、動態（自動化）性。**

這裡我們會談三種類型的負載平衡方式，DNS、GSLB、單純的 LB。
接著再談到相應的 *資源管理* 和 *網路設定* 的注意事項。

#### DNS

只有人才能可以決定現在這個服務要用什麼領域名稱（domain name），而這個名稱通常是不會改變的。
當這個域名被決定了，就可以註冊進 DNS 中。
這裡要注意的是網域名稱可以是唯一，但是目的地位置可能是分散在世界各地。

但是對於服務提供者來說，即使送出多個 IP 位置，最終客戶選擇要使用哪個 IP，是客戶決定的。
它可能是傳統的循環比對（round robin），或者依照瀏覽器的邏輯去判斷，這時要利用這個機制去做到彈性的負載平衡，就不太實際了。

:::info
這裡所說的彈性，是指當服務降載甚至失能了，就需要避免客戶再送請求過來。
這時，彈性且快速的阻止每個 DNS 發送這個失能的 IP 是困難的。
因為 DNS 的運作是複雜（可以說是[網路世界中最複雜的一塊](https://www.amazon.com/Network-Warrior-Everything-Need-Wasnt/dp/1449387861)，想想那些路由的協定，RIP、EIGRP、OSPF、BGP）且不會是任何一個統一單位管理的。
:::

#### GSLB

對於這種不同地區的負載平衡，更常見的是提供多個 GSLB。
GSLB 可以做到檢查下游的健康狀況、彈性分配流量到不同節點，
最重要的是它通常歸你所管，所以你可以快速對他進行任何設定（設定錯了就會[死很快](https://zh.wikipedia.org/zh-tw/2021年Facebook當機事件)）。

如果你有多個 GSLB（多個資料中心），你心裡要有個底：並不是每次請求，客戶都會乖乖照著前次的 GSLB 進來。

#### 負載平衡器

這其實和 GSLB 的工作職責很像，只是 GSLB 通常是任何外部的人都連得到，但是這裡的負載平衡器是指某服務（或多個服務）前面的平衡器。

如果這個平衡器是負責多個服務的，他就會有很多 VIP(s)，然後每個 IP 對應一個服務（多節點）。

![DNS, GSLB, LB 和服務的組合圖](https://i.imgur.com/vkaOAfG.png)

這裡平衡器其實和 GSLB 一樣都需要設定：

-   平衡負載的演算法選擇。
-   如何檢查各個節點是否正常（health check）。
-   是否要讓特定使用者連到特定節點，即所謂的 sticky-session。
-   當服務失能時，要怎麼回應

對於平衡器來說，不再是以[主機名稱](#網路)（hostname）來做搜尋名稱，而是外部網址（domain，其實本來就是這樣，只是這邊再強調一次 *hostname* 跟 *domain* 的差異）。
除此之外，有時他不是用來做「負載平衡」而是服務引導，例如 HTTP 路徑為 `/login` 走這、`/profile` 走那。

#### 資源的需求控制

流量增長可能會耗盡系統的資源，以網路為例，有幾點要注意：

-   Socket 會被耗盡，並需要等待舊的被關閉（關閉前會需要進入 [`TIME_WAIT`][tcp-qst] 狀態）。
-   一部分封包進來需要等待所有封包進來，這時記憶體就會被這些不完整的封包佔用。
-   以 TCP 為例，建立連線前會進入 `listen` socket 的佇列，只有成功建立連線的 socket 才會開始移交給應用程式端。

當上述行為踩到限制，就會開始拒絕進來的請求，進而促發請求端[重新嘗試連線或重新發送資料的機制][retry]，加重服務的負擔。
除此之外，一個連線會歷經很多階段的處理，這時如果應用層端的服務已經滿載了，我們當然**會希望請求在很早的階段就被回拒**。

一個健全的負載平衡器，就很適合在最一開始就拒絕請求，減輕服務的負擔。
這時，應用程式的健康檢查就很重要了，提供完整的資訊，讓負載平衡器有能力判斷丟進去的量。
除此之外，也可以使用一些[適應性並行處理][adpt]的機制。
最後這個回應可以是單純的 *HTTP 503 Service Unavailable*。

#### 網路相關的注意事項

這裡有幾個面向可以考慮：

-   *路由*，路由是網路世界最複雜的一塊，不是三言兩語說得盡（當然[有很多相關的書](https://www.networkstraining.com/best-computer-networks-textbooks/)）。
  隨著節點的增加，需要維護路由表，不管這個路由表是資料庫或是一個表格。
  近期也慢慢興起全部由軟體控制的架構，例如虛擬交換器、K8s、VLAN tagging 等。
  這些路由設定包括單一節點有多個出路口（一個給服務、一個給維運等等）的複雜狀況。
-   *服務發現*，每個提供這個機制的軟體（ZooKeeper、etcd 等等）都有自己的權衡機制，適合不同場景，不要認為這些軟體都是一樣的。
-   *VIP 的轉換*，負載器會透過 VIP 來指定服務要走的節點（HA, active/standby 的機制），
  這時要注意，當 VIP 進行切換時，每個 TCP 連線都可能在傳遞下一個封包時失能，這包括那些重要的資料庫連線。

### 叢集管理

叢集管理並不是一個簡單的公式，把數字代進去就可以得到結果。
我們需要考量自己的需求和環境（迭代率、即時性、維運性），來決定我們需要補足哪個面向的不足。

叢集管理工具是用來減輕人類負擔，
所以當一個人類因為錯誤操作導致叢集失能，
我們應該歸咎於工具的偵錯性和管理性的失能。
但這也回應到叢集管理並不是魔法，
他仍然是一行一行的程式碼，
所以在看到一些文章或新聞在推廣某個管理工具（例如 K8s）時，
應首先思考這工具是否符合需求，
建置、維運、拆除（工具一定會有迭代）的成本和其帶來的效益。

:::spoiler AWS 的失能案例
[AWS 2017 年發生的 S3 失能事件](https://aws.amazon.com/message/41926/)的屍檢報告中可以看到：
*一位授權的管理者根據指南進行操作，在執行一個關閉單一節點的指令時，因為指令的錯誤，導致關閉了大部分的節點*。
在此我們可以反思幾件事情：

-   文中從沒出現「人為錯誤」這類相關訊息，因為人類犯錯是可以被預期和接受的，身為一個管理工具卻沒能感知到這件事情。
  所以文中強調的是管理工具的失能。
-   操作指南代表以前有人照著這些指令執行，但是為什麼以前沒有發生意外？
  我們常常檢視那些失敗的案例，但我們也可以去檢視那些成功的案例。
  例如之前有人也寫錯過，被在提交前的某個流程上其他人審核出來了。
-   自動化的反應是快速的，在屍檢報告中 AWS 最終降低了移除節點的速度和增加一些保安系統
  （當減少的量低於系統當前承載的量時提出警告）。
:::

我們會先釐清「叢集管理和被管理的服務」之間的關係，
接著試著讓「服務透明化」也就是利於管理。
當服務需要的節點數越來越多時，
需要一些「備置和部署」的自動化工具協助管理，
最後有些服務不適合快速重啟，需要一個「控制管理的介面」。

#### 平臺和使用者的關係

分別以監控系統和資料庫來檢視一下現在的軟體環境中，平台建置者和開發人員之間的關係。

現在有很多開源的監控系統，當你把平台建置（可能是系統工程師）起來之後，是怎麼讓開發人員使用的？

早期可能是開發人員填單子，請相關人員做好應用程式的監控。
但隨著發展（DevOps），開發者也慢慢開始自己設定和調整相關監控。
這有點像是寫程式時的介面（interface），建置人員做好一個彈性很高的平台後，讓開發人員填好自己的實作。

這代表之中的責任移轉了，平台建置人員專注於多樣化、穩定和有效率的平台，
開發人員專注於應用邏輯，調整水位、示警閥值等等。

同樣的狀況發生在資料庫中，DBA（Database Architect）的工作應該是建立一個高效率和穩定的資料庫。
但是早期 RDBMS 的系統下，應用程式的設計邏輯會大大影響資料庫的穩定度，
慢慢的 DBA 就變成 DBA(Database Administrator)，開始要管理應用程式的資料庫邏輯。

這也是 NoSQL 運動的背景因素之一，嘗試要把資料庫管理和應用邏輯抽離。

這些歷史知識，都可以幫助我們暸解，一個叢集管理工具應該要長成什麼樣子，
我們不仿把 Kubernetes 套用在這個關係之中，然後思考一下它現在的樣子是一個理想的樣子嗎？

:::info
作者提供一個思考點：

如果你發現你的工作是每天（或每隔幾天）都有個固定事情要做，例如重啟服務。
這就是一個很強的論點說明現行的工具已經不適合使用了。
:::

#### 服務的透明化

要怎麼知道你的服務或 server 現在的健康狀況怎麼樣了？透明化你的服務。
監控系統百百種，早期每種類型的監控都需要付上大筆鈔票來購買企業的服務，但現在開源服務遍地開花，我們的基準是什麼？

-   是否給服務使用者（不是服務開發者）帶來好的體驗
-   能否替公司賺錢（省錢）

以這些為出發點，找到那些藏在細節的魔鬼，例如：

-   找出 bottleneck，例如最常用的連線，可以 cache 嗎、應用邏輯可以 glob 嗎？
-   是否有佇列現象
-   每次前端開發是否都一樣，例如 [golden tookit](https://pub.dev/packages/golden_toolkit)

依照這些東西，就可以去設計我該收集哪些日誌、指標和示警。
同時還要考量成本：開發、建置、基礎設施、維運和效率（設定優化）。
這些都是以目標（賺錢、省錢，好用、穩定）為思考點，回扣到做法，
而非從技術層面為立足點。

:::info
以 vip auth 為例：

-   減少流量（cache option）
-   減少錢（convergent log, decrease trace ratio）
-   減少潛時（glob access token scope）
-   開發 tracing server（OpenTelemetry）？
:::

要注意每個團隊要看的東西可能不一樣：

-   開發人員可能想看 log 檢查奇怪的程式行為
-   分析人員可能想看使用者整體的行為
-   專案的管理人員可能想看功能的使用狀況

身為一個監控系統要怎麼滿足這些東西？
[串流處理](https://evan361425.github.io/feedback/designing-data-intensive-applications/derived-stream/#_3)。

另外在選擇哪些資訊要暴露時，通常是所有東西都給他暴露出來，
實際在做維運的時候，當發生想要的資訊沒有的時候，
就只能看到兩個工程是相視而笑，兩手一攤。
這裡整理一些要資訊的種類：

-   流量，請求數、併行數。
-   使用者，登入失敗、在線數。
-   商務邏輯，成交數、入帳額。
-   資料庫，連線、請求失敗數、回應時間
-   運算資源，連線池、線程池、阻塞狀態。
-   儲存資源，記憶體、快取

這些資訊通常都會搭配時間軸（例如，近兩個小時的狀態）和閥值（超過 80% 就開始通知管理者）。

#### 備置和部署

隨著應用程式或者節點數量的增加，需要讓每次更新或異動可以快速、便捷。

在服務備置（provision）時期，你通常會有推（push）或者拉（pull）這兩種模式。
「推」會是一個中央服務，把資訊（組態設定、鏡像檔、執行檔等等）送到指定節點或服務中，
這種做法比較單純，可以透過 SSH 等機制快速達成驗證授權行為。
「拉」則是讓各個節點去拉取指定位置的資訊，這通常對於快速生滅的環境（例如容器化）很適合，
對於擴增性（scalability）也有很好的輔助。
但缺點就是需要設計好驗證授權的機制。

除此之外，在備置時通常會希望環境是乾淨的，可重複再現的
（今天備置和明天備置的結果要一樣，可以透過 Lock 檔）。
也就是每次備置時都希望通過單元或整合測試。

除了靜態設定，你可以透過提供組態設定的服務（通常是 *ZooKeeper* 或 *etcd*）來達到動態設定。
在這之中，需要注意幾個要點：

-   當組態服務失能時，應用程式、節點要可以正常運作。
-   確保組態服務沒有能力可以快速終結大量節點的能力。
-   資料要備份。

#### 即時控制你的應用程式

有時候服務沒辦法快速啟動，
例如從快取暖機、建置程式碼的虛擬機（例如 JVM）、服務住在虛擬機上等等。
這時，你需要一個方式可以在外部影響應用程式，例如：

-   重置迴圈
-   調整連線池的數量和逾時
-   暫停和某個服務的連線
-   重新讀取設定檔
-   某個功能的開關
-   開始、關閉對外服務

但是不要在這裡去暴露「改動資料庫的綱目」或「清除快取」的接口，
因為這種即時調動大量持續性狀態，通常都會造成一些意想不到的邊際狀況。

還有個問題是這個接口要怎麼暴露？
通常會選擇打開一個 HTTP 端口，透過打 HTTP API 來達成目的。
但如果節點有五百個，難道要對五百個節點打 HTTP 請求嗎？

這時我們可以使用事件佇列機制，讓這五百個節點去監聽某個事件，
然後管理人員發出這個事件，讓每個節點一批一批（一次性讓大家做事會增加大量負荷）的去處理這個事件。
有時大家會寫一個 GUI 介面讓大家可以輕鬆操作，聽起來好像很合理，
但是介面只能提供高層次的操作，例如讓審核者同意這次自動化的操作等等。
否則每次讓人用滑鼠去點擊，無疑是增加失誤的風險。

#### 小節

就像[尚未出現的整合資料的服務](https://evan361425.github.io/feedback/designing-data-intensive-applications/farewell/#_11)一樣，
對於應用程式來說，我們還是痛苦於整合所有驗證授權系統和監控介面。

### 維運

維運

:::info
Virtual IP；虛擬 IP 位置，會需要有個服務管理 VIP 對應真實 IP 的表格。

虛擬 IP 通常有幾個功能：

-   用作單一服務多個節點的唯一路口，
-   HA 機制，
-   單純在私有網路遮罩下的 IP 分配。

由於功能很多，需要注意上下文中其代表的意義。
:::

---

[retry]: https://evan361425.github.io/essay/web/retry-strategy/
[adpt]: https://evan361425.github.io/feedback/adaptive-concurrency/
[tcp]: https://evan361425.github.io/essay/web/tcp/
[tcp-qst]: https://evan361425.github.io/essay/web/tcp/#_7

*[NAS]: Network Attach Storage；透過網路把資料放在其他地方
*[SAN]: Storage Area Network；透過線把資料放在其他地方
*[NIC]: Network Interface Control；網路介面控制，或者稱為網卡
*[DNS]: Domain Namer Server；用來把網域名稱轉成 IP 的服務
*[VIP]: Virtual IP；虛擬 IP 位置，詳見文末。
*[GSLB]: Global Server Load Balance；全域負載，通常是一個地理位置的資料中心放一個，做為看守大門的領路人。
